---
layout: default
title: Object detection using YOLO
permalink: /YOLO/
---
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Object detection using YOLO</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
</head>

<body>
    <div id="header">
        <h1>YOLOv8 Object Detection Example</h1>
        <p>Serving : <code class="code">yolov8n.tfjs</code>, Size : <code class="code">640x640</code></p>
    </div>

    <header>
        <nav class="nav-wrapper">
            <div class="container">
                <ul>
                    <il><button id="runInference">Run Inference</button></il>
                    <il><button id="stopInference">Stop Inference</button></il>
                </ul>
            </div>
        </nav>
    </header>

    <div id="main" style="position: relative;">
        <video id="webcam" autoplay playsinline width="640" height="480"></video>
        <canvas id="outputCanvas"></canvas>
    </div>
    

    <script>
        const classNames = {
            0: "person", 1: "bicycle", 2: "car", 3: "motorcycle", 4: "airplane", 5: "bus", 6: "train", 7: "truck",
            8: "boat", 9: "traffic light", 10: "fire hydrant", 11: "stop sign", 12: "parking meter", 13: "bench",
            14: "bird", 15: "cat", 16: "dog", 17: "horse", 18: "sheep", 19: "cow", 20: "elephant", 21: "bear",
            22: "zebra", 23: "giraffe", 24: "backpack", 25: "umbrella", 26: "handbag", 27: "tie", 28: "suitcase",
            29: "frisbee", 30: "skis", 31: "snowboard", 32: "sports ball", 33: "kite", 34: "baseball bat",
            35: "baseball glove", 36: "skateboard", 37: "surfboard", 38: "tennis racket", 39: "bottle",
            40: "wine glass", 41: "cup", 42: "fork", 43: "knife", 44: "spoon", 45: "bowl", 46: "banana",
            47: "apple", 48: "sandwich", 49: "orange", 50: "broccoli", 51: "carrot", 52: "hot dog", 53: "pizza",
            54: "donut", 55: "cake", 56: "chair", 57: "couch", 58: "potted plant", 59: "bed", 60: "dining table",
            61: "toilet", 62: "tv", 63: "laptop", 64: "mouse", 65: "remote", 66: "keyboard", 67: "cell phone",
            68: "microwave", 69: "oven", 70: "toaster", 71: "sink", 72: "refrigerator", 73: "book", 74: "clock",
            75: "vase", 76: "scissors", 77: "teddy bear", 78: "hair drier", 79: "toothbrush"
        };

        const TARGET_WIDTH = 640;
        const TARGET_HEIGHT = 640;
        let model;
        let isRunning = false;
        let inferenceInterval;

        async function loadModel() {
            tf.setBackend('webgl');
            model = await tf.loadGraphModel('/assets/YOLO/YOLO_model.json');
        }

        async function runModel(tensor) {
            if (!model) await loadModel();
            return model.predict(tensor);
        }

        async function processWebcamFrame() {
            const video = document.getElementById('webcam');
            const tensor = await webcamToTensor(video);
            const startTime = performance.now();
            const predictions = await runModel(tensor);
            const endTime = performance.now();
            const inferenceTime = endTime - startTime;
            console.log(`Inference Time: ${inferenceTime.toFixed(2)} ms`);
            const detections = processPredictions(predictions, classNames);
            await drawBoundingBoxes(video, detections);
        }

        function extractSelectedPredictions(indices, boxes, labels, classNames) {
            return indices.map(i => {
                const box = boxes.slice([i, 0], [1, -1]).squeeze().arraySync();
                const label = labels.slice([i], [1]).arraySync()[0];
                return { box, label: classNames[label] };
            });
        }

        async function webcamToTensor(videoElement) {
            const canvas = document.createElement('canvas');
            canvas.width = TARGET_WIDTH;
            canvas.height = TARGET_HEIGHT;
            const ctx = canvas.getContext('2d', { willReadFrequently: true });

            ctx.drawImage(videoElement, 0, 0, TARGET_WIDTH, TARGET_HEIGHT);
            const imageData = ctx.getImageData(0, 0, TARGET_WIDTH, TARGET_HEIGHT);
            const tensor = tf.browser.fromPixels(imageData);
            return tf.cast(tensor, 'float32').div(tf.scalar(255)).expandDims(0);
        }

        function processPredictions(predictions, classNames) {
            return tf.tidy(() => {
                const transRes = predictions.transpose([0, 2, 1]);
                const boxes = calculateBoundingBoxes(transRes);
                const [scores, labels] = calculateScoresAndLabels(transRes, classNames);
                const indices = tf.image.nonMaxSuppression(boxes, scores, predictions.shape[2], 0.45, 0.2).arraySync();
                return extractSelectedPredictions(indices, boxes, labels, classNames);
            });
        }

        function calculateBoundingBoxes(transRes) {
            const [xCenter, yCenter, width, height] = [
                transRes.slice([0, 0, 0], [-1, -1, 1]),
                transRes.slice([0, 0, 1], [-1, -1, 1]),
                transRes.slice([0, 0, 2], [-1, -1, 1]),
                transRes.slice([0, 0, 3], [-1, -1, 1])
            ];

            const topLeftX = tf.sub(xCenter, tf.div(width, 2));
            const topLeftY = tf.sub(yCenter, tf.div(height, 2));
            return tf.concat([topLeftX, topLeftY, width, height], 2).squeeze();
        }

        function calculateScoresAndLabels(transRes, classNames) {
            const rawScores = transRes.slice([0, 0, 4], [-1, -1, Object.keys(classNames).length]).squeeze(0);
            return [rawScores.max(1), rawScores.argMax(1)];
        }

        async function drawBoundingBoxes(imageElement, detections) {
            const canvas = document.getElementById('outputCanvas');
            const ctx = canvas.getContext('2d', { willReadFrequently: true });
            canvas.width = imageElement.width;
            canvas.height = imageElement.height;
            ctx.clearRect(0, 0, canvas.width, canvas.height);

            const resizeScale = Math.min(TARGET_WIDTH / canvas.width, TARGET_HEIGHT / canvas.height);
            const dx = (TARGET_WIDTH - canvas.width * resizeScale) / 2;
            const dy = (TARGET_HEIGHT - canvas.height * resizeScale) / 2;

            detections.forEach(({ box, label }) => {
                const [x, y, width, height] = box;
                ctx.strokeStyle = 'green';
                ctx.lineWidth = 2;
                ctx.strokeRect(x * resizeScale + dx, y * resizeScale + dy, width * resizeScale, height * resizeScale);
                ctx.fillStyle = 'green';
                ctx.font = '18px Arial';
                ctx.fillText(label, x * resizeScale + dx, y * resizeScale + dy);
            });
        }

        function startInferenceLoop() {
            inferenceInterval = setInterval(processWebcamFrame, 100);
        }

        function stopInferenceLoop() {
            clearInterval(inferenceInterval);
        }

        document.getElementById('runInference').addEventListener('click', () => {
            if (!isRunning) {
                isRunning = true;
                startInferenceLoop();
            }
        });

        document.getElementById('stopInference').addEventListener('click', () => {
            if (isRunning) {
                isRunning = false;
                stopInferenceLoop();
            }
        });

        async function setupWebcam() {
            const video = document.getElementById('webcam');
            const stream = await navigator.mediaDevices.getUserMedia({
                video: true
            });
            video.srcObject = stream;
        }

        setupWebcam();
    </script>
</body>

</html>